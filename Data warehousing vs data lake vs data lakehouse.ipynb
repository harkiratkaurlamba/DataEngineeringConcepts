{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e9deb04-08d3-4573-b45e-2af40cc942ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Schema-on-read** is a data management technique that _allows data to be stored and processed without defining a schema beforehand_. It's an alternative to the traditional schema-on-write method. \n",
    "\n",
    "_How it works?_\n",
    "\n",
    "- Schema-on-read allows raw data to be stored in a data lake without any transformations. When the data is read for analysis, the schema is applied. This approach supports flexible data models and is ideal for unstructured data. \n",
    "- Benefits of schema-on-read: \n",
    "- Faster: Schema-on-read is faster because the schema doesn't need to be defined before loading. \n",
    "- More agile: Schema-on-read allows for single storage of data for many use cases. \n",
    "- Scalable: Schema-on-read can scale up rapidly and is suitable for massive volumes of unstructured data. \n",
    "\n",
    "**Use cases**: \n",
    "- Schema-on-read is associated with the rise of data lakes and is used for data science use cases and machine learning models. \n",
    "- It's also used in the Hadoop Distributed File System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab18e024-f146-469b-bae8-1a3d992d2469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Data warehouse**: It is a repo where we can centralize, store, and manage large amounts of data from multiple data sources to serve the company's analytics workload.\n",
    "\n",
    "challenges: \n",
    "Data does not only come in tabular format; it can also be video, audio, or text documents. Unstructured data caused massive trouble for the relational data warehouse, which handles structured data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1866d14-6cda-421f-81a6-db700dc3eb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The data lake** : storing a vast amount of data in its format (in HDFS or later in cloud object storage). Unlike traditional data warehouses, the data lake doesn’t require us to define the schema beforehand, so all data, including unstructured ones, can be stored in the lakes without concern about the constraint format.\n",
    "\n",
    "challenges:\n",
    "People tried to replace the data warehouse with the data lake by bringing the processing right on top of the lake. However, the approach had many serious drawbacks; the data lake soon became the data swamp due to lacking proper data management features from the warehouse, such as data discovery, data quality and integrity guarantee, ACID constraints, and data DML support.\n",
    "\n",
    "**Thus, combining the data lake and the data warehouse is the better option.**\n",
    "\n",
    "The lake is still where we can ingest raw data in any format without the care of the predefined schema. Later, a subset of data is transformed and loaded into the warehouse system for reporting and analysis. Advanced use cases like machine learning can still access raw data in the data lakes.\n",
    "\n",
    "Maintaining the above two-tier data architecture poses some challenges:\n",
    "\n",
    "- **Data staleness**: The data in the warehouse is stale compared to the lake’s data. This is a step back from the original data warehouse architecture, where new operational data was immediately available for analytics demands.\n",
    "\n",
    "- **Reliability**: Consolidating the data lake and warehouse is difficult and costly, requiring engineering effort to ETL data between the two systems. Reconcilliation checks?? remeber? your literally 2,5 years of life went by to do it?\n",
    "\n",
    "- **Limited support for advanced analytics**: Initially, data warehouses did not support machine learning workloads well, as such tasks require processing large datasets with complex programmatic code. Vendors often recommended exporting data to files, which further increased the complexity of the overall system. Alternatively, users could run these workloads directly on data lake data stored in open formats. However, this approach often came at the cost of rich management features data warehouses provide, such as ACID transactions or data versioning. (This may no longer be the case, as modern data warehouse solutions like BigQuery now offer efficient capabilities for handling machine learning workloads directly within their interfaces.)\n",
    "\n",
    "- **Total cost of ownership**: Users are billed twice the storage cost for data duplication in the data lake and warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8a4182e-3233-4478-a9b8-2895fe45a0ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Data lakehouse**\n",
    "\n",
    "It is an architecture based on low-cost storage (e.g., object storage) that enhances traditional analytical DBMS management and performance features such as ACID transactions, versioning, caching, and query optimization. The Lakehouse combines the best of both worlds.\n",
    "\n",
    "The difference from the past effort, when people also tried to bring processing right on top into the data lake, is that more efficient metadata layers were introduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78f3cd83-4871-454f-beb3-e98c8fec6de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Databricks created the **_Delta Lake_**\n",
    "- Netflix created the **_Iceberge_** to manage analytics data more efficiently on S3\n",
    "- Uber developed **_Hudi_** to bring the capability of data upsert and incremental processing to the data lake.\n",
    "\n",
    "> In Lakhouse, every data-related operation must go through these open table formats to enable data warehouse features like \n",
    "- table snapshotting\n",
    "- time traveling\n",
    "- ACID\n",
    "- schema evolution\n",
    "- partition evolution. \n",
    "\n",
    "These table formats also record statistics that help the query engine prune unnecessary data (e.g., the min/max column values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c56880b7-0afe-4ac9-b075-30a68c7efdae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data warehousing vs data lake vs data lakehouse",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
