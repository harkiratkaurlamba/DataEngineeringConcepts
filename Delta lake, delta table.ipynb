{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37cda76c-548f-462d-823b-a31487ce794b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**ACID**\n",
    "\n",
    "- **Atomicity** - each statement in a transaction (to read, write, update or delete data) is treated as a single unit. Either the entire statement is executed, or none of it is executed. This property prevents data loss and corruption from occurring if, for example, if your streaming data source fails mid-stream.\n",
    "- **Consistency** - ensures that transactions only make changes to tables in predefined, predictable ways. Transactional consistency ensures that corruption or errors in your data do not create unintended consequences for the integrity of your table.\n",
    "- **Isolation** - when multiple users are reading and writing from the same table all at once, isolation of their transactions ensures that the concurrent transactions don't interfere with or affect one another. Each request can occur as though they were occurring one by one, even though they're actually occurring simultaneously.\n",
    "- **Durability** - ensures that changes to your data made by successfully executed transactions will be saved, even in the event of system failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38b73d1c-e95a-4c41-9424-8e57e307bcf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cloud Storage started to replace the HDFS file for the Data Lake implementation. Its advantages include theoretically unlimited scale, pay-as-you-go billing, durability, and reliability. As a result, many organizations now use cloud object stores to manage large structured datasets in data warehouses and data lakes. \n",
    "\n",
    "Popular open-source systems, including Apache Spark or Presto, support reading and writing to cloud object stores using file formats such as Apache Parquet and ORC. \n",
    "\n",
    "Cloud services, including Google BigQuery and Redshift Spectrum, can also query directly against these systems and these open file formats.\n",
    "\n",
    "Object storage has shortcomings when putting it into the Lakehouse context, which requires data warehouse capabilities right on top of the Datalake. \n",
    "\n",
    "Most cloud object storages are implemented as key-value stores, making achieving ACID transactions and high performance challenging: operations such as listing objects are expensive, and consistency guarantees are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c666e73-ddff-4306-87d1-e2fd6afc5fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Assume we store relational data natively in the cloud object storage. We logically considered each table to be stored as a set of Parquet file objects; this approach creates correctness and performance challenges for more complex workloads:\n",
    "\n",
    "- There is no isolation between queries: if a query needs to update multiple objects in the table, consumers will see partial updates as the query updates each object individually. here atomicity and consistency breaks(ACID)\n",
    "- If an update query fails in the middle, the table is corrupted. durability fails here\n",
    "- Metadata operations are expensive in large tables with millions of objects. (e.g., list)\n",
    "\n",
    "ex: humare table m ek id h: 8097, hume iska status update krna h from open to closed\n",
    "\n",
    "iske liye we have to scan all of the files of this table to find out this id, or atleast a bucket of ids ki files\n",
    "\n",
    "haina, aur firr isme latest record find krna and update krna. \n",
    "\n",
    "now assume there are 2 ids, 8097 and 1023.\n",
    "\n",
    "ho sakta h 8097 ekk file m ho aur 1023 kisi aur file m. \n",
    "1. file open\n",
    "2. find data\n",
    "3. if found update and close file\n",
    "4. do we have more ids to update? if yes repeat step 1\n",
    "5. if not data point found: close and open next file, go to step1 and repeat the process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf459ee-5052-436b-ba56-417b0fb29089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Delta lake** :an ACID table storage layer on cloud object storage. The core idea of Delta Lake is straightforward:\n",
    "\n",
    "> Maintain information about which objects belong to a Delta table in an ACID manner, using a write-ahead log in the cloud object store. The objects are encoded in Parquet, making it easy to write connectors from engines that can process Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ea785d-fadf-44de-ae01-5e8064884f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta lake, delta table",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
