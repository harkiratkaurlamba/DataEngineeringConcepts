{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "638c9f19-4c5b-417f-a820-cb029e76bc8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "kafka connect makes it easy to connect data from numerous sources into kafa & steam data out of kafka to numerou targets. \n",
    "kafka connect i lfexible, you can choose to run kafka connect yourself or take advantage of fully managed connectors provided by confluent cloud.\n",
    "\n",
    "confluent cloud also provide: fully manage kafka, schema registry , stream processing usin ksql.\n",
    "\n",
    "kafka connect runs it own process sperate from kafka brokers, it is distributed, scalable, and fault tolerant\n",
    "\n",
    "no proramming req, just configuration. ingress and egress and light weight transformation on data as it passes through\n",
    "\n",
    "use cases: \n",
    "1. ingest real time streams of event and stream to target to dbs for analytics\n",
    "kafka connect pulls dta for updates, and translats that info into real time events that it then produces to kafka.\n",
    "\n",
    "having kafka sit between source and target meas we are building loosely coupled system. additionally, kafka acts as a buffer to the data, applying back pressure as needed. since kafka is used, system is scalable and fault tolerant. because kafka stores data upto configurabke time interval\n",
    "per data entity or kafka topic, it is possible to stream same data to multiple downstream topic. we only need to ingest once and be used endless number of times/rquirements/technioogies\n",
    "\n",
    "\n",
    "2. an app produces series of logging events, and e want these events to be written in a logging store or persist in a db. apart form main business logic this would be an additional thing to do apart from sclaing up and making it fault tolerant. by moving data to kafka we are free to set up kafka connectors  to move data to any donstream data\n",
    "\n",
    "\n",
    "\n",
    "why use kafka?\n",
    "1. to serve as a message broker between independent services, \n",
    "2. to serve as a permanent system of records\n",
    "\n",
    "how make it do both: ustelize CDC change data capture, insert update and delete and move that into a stream of events in kafka. we can do this in real time by using the underlying db transaction logs and light weight queries\n",
    "\n",
    "ex:\n",
    "an order is accepted, any app subscribed to the stream of events woul be able to take actions based on thise events, ex: order fullfilment system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b9c7c34-16a6-4b6c-8be4-3c13babfac43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "event in kafka is modelled in key - value pair.\n",
    "\n",
    "internally inside kafka, keys and values are just streams of bytes(deserialized form)\n",
    "\n",
    "value(serialized): json/avro/json schema/protobuf ex: app doman object/raw input message/output of sensor etc.\n",
    "keys: can be complex domain obects like json/avro/json schema/protobuf orprimitive object. they are not unique  values like PK. key in kafka message is identifier of user/device/id etc..parallelization and data locality\n",
    "\n",
    "kafka objects: stored in key value pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd71f80b-0dc1-433e-af57-fa417ffd110c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "KAfka is collection of immutable append-only logs.\n",
    "primary component of storage in kafka is topic. Topics are broken down into smaller components called partitions. when you rite a message to kafka, that message actually gets stored in one of the topic's partition\n",
    "\n",
    "the partition to which message is routed to is based on thekey of that message.\n",
    "\n",
    "offset: In Kafka, an offset is a unique integer identifier assigned to each message within a partition, representing its position and enabling consumers to track their progress and resume processing from a specific point. \n",
    "When a producer publishes a message to a Kafka topic, the message is appended to the end of the partition's log and assigned a new, sequential offset. \n",
    "Consumers maintain their current offset, indicating the last processed message in each partition. \n",
    "This mechanism enables Kafka to provide sequential, ordered, and replayable data processing. \n",
    "\n",
    "Importance:\n",
    "Tracking Progress: Offsets allow consumers to track their progress within a partition, ensuring that they don't miss any messages. \n",
    "Resuming Processing: If a consumer crashes or restarts, it can resume processing from the last committed offset, ensuring that no data is lost. \n",
    "Scalability: Offsets enable Kafka to scale horizontally while maintaining fault tolerance\n",
    "Types of Offsets:\n",
    "Current Offset: The position of the next message that will be consumed by a consumer. \n",
    "Committed Offset: The last offset that a consumer has successfully processed and committed. \n",
    "Log-end Offset: The position corresponding to the last available message in a partition. \n",
    "\n",
    "Offset Management:\n",
    "Kafka stores consumer offsets in a special internal topic called __consumer_offsets. \n",
    "Consumers can either automatically commit offsets periodically or manually control the committed position using commit APIs. \n",
    "You can also use tools like kafka-consumer-groups.sh to reset consumer offsets. \n",
    "\n",
    "Consumer lag\tDifference between the committed offset and the last offset.\n",
    "Scalability\tDividing data into partitions and using offset to track progress is crucial to enabling parallel processing and scalability in Kafka architecture.\n",
    "Message ordering\tOffsets help consumers process messages in the same order they arrive in.\n",
    "Fault tolerance\tKafka offsets enable fault tolerance by allowing failed consumers to restart from the last message it processed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ways to write in kafka topic: ui, producer API, kafka connect, confluent cli\n",
    "\n",
    "A message/event in kafka is treated as a key-vlaue pair"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Kafka connect",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}